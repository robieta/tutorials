{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before we begin, we need to install torch if it isnâ€™t already available.\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "`conda install pytorch -c pytorch`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;or\n",
    " \n",
    "`pip install torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc\n",
    "\n",
    "We'll start by defining several helper functions which we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# We want to show certain threading effects, but 1 vs. several dozen\n",
    "# is often too stark a contrast.\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "\n",
    "def print_as_cpp(source: str):\n",
    "    display(Markdown(f\"```c++\\n{source}\\n```\"))\n",
    "\n",
    "\n",
    "def load_extension(name: str, code: str, fn_name: str):\n",
    "   \"\"\"Compile our implementation into an inline module.\n",
    "\n",
    "   Normally we would modify ATen instead, however this allows us\n",
    "   to show an example without having to build PyTorch from source.\n",
    "   \"\"\"\n",
    "   from torch.utils.cpp_extension import load_inline\n",
    "   return load_inline(\n",
    "      name,\n",
    "      code,\n",
    "      extra_cflags=[\"-O2\", \"-g\", \"-mavx2\", \"-mfma\"],\n",
    "      functions=[fn_name])\n",
    "\n",
    "def module_to_setup_str(m):\n",
    "   \"\"\"Handle importing `m` during Timer setup.\n",
    "\n",
    "   This step is only necessary because we are using custom extensions for\n",
    "   demonstration, rather than modifying and rebuilding PyTorch core.\n",
    "   \"\"\"\n",
    "   module_dir, module_name = os.path.split(m.__file__)\n",
    "   return textwrap.dedent(f\"\"\"\n",
    "      import sys\n",
    "      if not {repr(module_dir)} in sys.path:\n",
    "         sys.path.append({repr(module_dir)})\n",
    "      import {module_name[:-3]} as my_module\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: a specialized implementation of `x + 1`\n",
    "\n",
    "In this tutorial, we are going to define the `shift` function, and show how to take a systematic approach towards optimizing it. For simplicity, we will only consider float Tensors on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c++\n",
       "\n",
       "// First attempt at a specialized implementation of `x + 1`\n",
       "at::Tensor shift(const at::Tensor & x) {\n",
       "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
       "\n",
       "    auto y = x.clone(at::MemoryFormat::Contiguous);\n",
       "    auto y_ptr = x.data_ptr<float>();\n",
       "    auto n = y.numel();\n",
       "    for (int i = 0; i < n; i++) {\n",
       "        *(y_ptr + i) += 1;\n",
       "    }\n",
       "    return y;\n",
       "}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift_impl_v0_src = \"\"\"\n",
    "// First attempt at a specialized implementation of `x + 1`\n",
    "at::Tensor shift(const at::Tensor & x) {\n",
    "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
    "\n",
    "    auto y = x.clone(at::MemoryFormat::Contiguous);\n",
    "    auto y_ptr = x.data_ptr<float>();\n",
    "    auto n = y.numel();\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        *(y_ptr + i) += 1;\n",
    "    }\n",
    "    return y;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print_as_cpp(shift_impl_v0_src)\n",
    "shift_impl_v0 = load_extension(\"shift_impl_v0\", shift_impl_v0_src, \"shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive benchmark: timeit.Timer\n",
    "\n",
    "### Note: this is just here as a placeholder to help me organize my thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native\n",
      "        n = 1     n = 1024    n = 16384\n",
      "---------------------------------------\n",
      "       8.3 us       8.9 us      14.5 us\n",
      "       8.2 us       8.8 us      14.4 us\n",
      "       8.4 us       8.7 us      13.8 us\n",
      "       8.3 us       8.5 us      14.0 us\n",
      "       8.1 us       8.6 us      14.1 us\n",
      "\n",
      "\n",
      "C++ Extension\n",
      "        n = 1     n = 1024    n = 16384\n",
      "---------------------------------------\n",
      "       4.1 us       5.1 us      17.8 us\n",
      "       4.2 us       5.1 us      21.7 us\n",
      "       4.1 us       5.0 us      17.9 us\n",
      "       4.1 us       5.0 us      18.4 us\n",
      "       4.1 us       5.1 us      20.7 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "repeats = 5\n",
    "sizes = (1, 1024, 16384)\n",
    "\n",
    "\n",
    "def measure_native(n):\n",
    "    num_runs, total_time = timeit.Timer(\n",
    "        \"x + 1\", \n",
    "        setup=f\"import torch;x = torch.ones(({n},))\",\n",
    "    ).autorange()\n",
    "    return total_time / num_runs\n",
    "\n",
    "\n",
    "def measure_cpp(n):\n",
    "    num_runs, total_time = timeit.Timer(\n",
    "        \"shift(x)\", \n",
    "        setup=f\"import torch;x = torch.ones(({n},))\",\n",
    "        globals={\"shift\": shift_impl_v0.shift},\n",
    "    ).autorange()\n",
    "    return total_time / num_runs\n",
    "\n",
    "\n",
    "for title, measure_fn in ((\"Native\", measure_native), (\"\\n\\nC++ Extension\", measure_cpp)):\n",
    "    print(f\"{title}\\n\" + \"\".join([f\"n = {i}\".rjust(13) for i in sizes]) + \"\\n\" + \"-\" * 13 * len(sizes))\n",
    "    for _ in range(repeats):\n",
    "        result_line = \"\"\n",
    "        for n in sizes:\n",
    "            result_line += f\"{measure_fn(n) * 1e6:10.1f} us\"\n",
    "        print(result_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime aware: torch.utils.benchmark.Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f46e18717f0>\n",
      "x + 1\n",
      "  11.19 us\n",
      "  1 measurement, 100 runs , 1 thread \n",
      "\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f46e579e588>\n",
      "x + 1\n",
      "  8.30 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "timer = Timer(\n",
    "    stmt=\"x + 1\",\n",
    "    setup=\"x = torch.ones((1,))\",\n",
    ")\n",
    "\n",
    "# The torch utils Timer returns a Measurement object, which contains\n",
    "# metadata about the run as well as replicates, if applicable.\n",
    "print(timer.timeit(100), \"\\n\")\n",
    "\n",
    "\n",
    "m = Timer(\n",
    "    stmt=\"x + 1\",\n",
    "    # Like timeit.Timer, initialization can be done using `setup=...` or `globals=...` (or both).\n",
    "    globals={\"x\": torch.ones((1,))},\n",
    "    \n",
    "    # torch.utils.benchmark.Timer takes several additional annotation argument:\n",
    "    #   label, sub_label, description, and env\n",
    "    # These change the __repr__ measurements, and are used when grouping and displaying\n",
    "    # measurements. (Discussed later.)\n",
    "    label=\"Add one\",\n",
    "    sub_label=\"Generic implementation.\",\n",
    ")\n",
    "\n",
    "print(timer.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer.blocked_autorange\n",
    "### A mixture of timeit.Timer.repeat and timeit.Timer.autorange\n",
    "\n",
    "While `timeit.Timer.autorange` takes a single continuous measurement of at least 0.2 seconds, `torch.utils.benchmark.blocked_autorange` takes many measurements whose times total at least 0.2 seconds (which can be changed by the `min_run_time` parameter) subject to the constraint that timing overhead is a small fraction of the overall measurement. This is acomplished by first running with an increasing number of runs per loop until the run time is much larger than measurement overhead (which also serves as a warm up), and then taking measurements until the target time is reached. This has the useful properties that it wastes less data, and allows us to take statistics in order to assess the reliability of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f46e182b9b0>\n",
      "x + 1\n",
      "  Median: 8.34 us\n",
      "  IQR:    0.28 us (8.23 to 8.51)\n",
      "  24 measurements, 1000 runs per measurement, 1 thread \n",
      "\n",
      "Mean:      8.6 us\n",
      "Median:    8.3 us\n",
      "IQR:       0.3 us\n",
      "Times:  [8.493732661008834e-06, 8.615698665380478e-06, ..., 8.232343941926956e-06, 9.091291576623917e-06]\n"
     ]
    }
   ],
   "source": [
    "m = Timer(\n",
    "    stmt=\"x + 1\",\n",
    "    setup=\"x = torch.ones((1,))\",\n",
    ").blocked_autorange()\n",
    "\n",
    "# Results summarized by __repr__\n",
    "print(m, \"\\n\")\n",
    "\n",
    "# Helper methods for statistics\n",
    "print(f\"Mean:   {m.mean * 1e6:6.1f} us\")\n",
    "print(f\"Median: {m.median * 1e6:6.1f} us\")\n",
    "print(f\"IQR:    {m.iqr * 1e6:6.1f} us\")\n",
    "print(f\"Times:  {str(m.times[:2])[:-1]}, ..., {str(m.times[-2:])[1:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why runtime awareness matters\n",
    "It's very easy to accidentally make an apples-to-oranges comparizon, such as comparing measurements with different numbers of threads, or forgetting to CUDA synchronize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeit.Timer:                      111 us\n",
      "torch Timer:                       370 us\n",
      "torch Timer(num_threads=...):      106 us\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((1024, 1024))\n",
    "\n",
    "num_runs, total_time = timeit.Timer(\"x + 1\", globals={\"x\": x}).autorange()\n",
    "m0 = Timer(\"x + 1\", globals={\"x\": x}).blocked_autorange()\n",
    "m1 = Timer(\"x + 1\", globals={\"x\": x}, num_threads=torch.get_num_threads()).blocked_autorange()\n",
    "\n",
    "print(f\"timeit.Timer:                   {total_time / num_runs * 1e6:6.0f} us\")\n",
    "print(f\"torch Timer:                    {m0.mean * 1e6:6.0f} us\")\n",
    "print(f\"torch Timer(num_threads=...):   {m1.mean * 1e6:6.0f} us\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.utils.benchmark.Compare\n",
    "Easy comparison of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------- Shift operator ------------------------------------]\n",
      "                               |   1   |   16  |  256  |  1024  |  4096  |  16384  |  32768\n",
      "1 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  8.2  |  8.6  |  8.7  |  9.3   |  10.3  |   14.7  |   22.3\n",
      "      Custom C++ operator      |  4.1  |  4.2  |  4.5  |  5.3   |   8.2  |   18.6  |   35.3\n",
      "2 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  8.0  |  8.4  |  8.7  |  9.3   |  10.4  |   14.6  |   23.4\n",
      "4 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  7.9  |  8.7  |  8.9  |  9.3   |  10.5  |   14.6  |   23.8\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Compare\n",
    "\n",
    "results = []\n",
    "for n in [1, 16, 256, 1024, 4096, 16384, 32768]:\n",
    "    for num_threads in [1, 2, 4]:\n",
    "        setup=f\"x = torch.ones(({n},))\"\n",
    "        results.append(Timer(\n",
    "            \"x + 1\",\n",
    "            setup=setup,\n",
    "            num_threads=num_threads,\n",
    "            label=\"Shift operator\",\n",
    "            sub_label=\"Generic implementation.\",\n",
    "            description=str(n),\n",
    "        ).blocked_autorange())\n",
    "\n",
    "\n",
    "    results.append(Timer(\n",
    "        \"my_module.shift(x)\",\n",
    "        setup=(\n",
    "            module_to_setup_str(shift_impl_v0) +\n",
    "            setup\n",
    "        ),\n",
    "        # Custom C++ operator does not support parallelism.\n",
    "        num_threads=1,\n",
    "        label=\"Shift operator\",\n",
    "        sub_label=\"Custom C++ operator\",\n",
    "        description=str(n),\n",
    "    ).blocked_autorange())\n",
    "\n",
    "compare = Compare(results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With extra formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------- Shift operator ------------------------------------]\n",
      "                               |   1   |   16  |  256  |  1024  |  4096  |  16384  |  32768\n",
      "1 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  8.2  |  \u001b[2m\u001b[91m8.6\u001b[0m\u001b[0m  |  8.7  |  9.3   |   10   |  \u001b[92m\u001b[1m  15 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  22 \u001b[0m\u001b[0m\n",
      "      Custom C++ operator      |  \u001b[92m\u001b[1m4.1\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4.2\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4.5\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m5.3 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  8 \u001b[0m\u001b[0m  |    19   |    35 \n",
      "2 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  \u001b[92m\u001b[1m8.0\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.4\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.7\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m9.3 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 10 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  15 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  23 \u001b[0m\u001b[0m\n",
      "4 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  \u001b[92m\u001b[1m7.9\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.7\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.9\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m9.3 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 11 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  15 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  24 \u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: this was me just playing with TensorAccessor so I could test some stuff. I'm sure you'll have critiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c++\n",
       "\n",
       "// First attempt at a specialized implementation of `x + 1`\n",
       "at::Tensor shift(const at::Tensor & x) {\n",
       "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
       "    \n",
       "    // Use TensorAccessor to handle stride calculation.\n",
       "    // This lets us skip copying `x`.\n",
       "    auto flat_x = x.flatten();\n",
       "    auto x_accessor = flat_x.accessor<float, 1>();\n",
       "\n",
       "    auto y = at::empty(x.sizes(), x.options());\n",
       "    auto y_ptr = y.data_ptr<float>();\n",
       "    \n",
       "    auto n = y.numel();\n",
       "    for (int64_t i = 0; i < n; i++) {\n",
       "        *(y_ptr + i) = x_accessor[i] + 1;\n",
       "    }\n",
       "    return y;\n",
       "}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift_impl_v1_src = \"\"\"\n",
    "// First attempt at a specialized implementation of `x + 1`\n",
    "at::Tensor shift(const at::Tensor & x) {\n",
    "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
    "    \n",
    "    // Use TensorAccessor to handle stride calculation.\n",
    "    // This lets us skip copying `x`.\n",
    "    auto flat_x = x.flatten();\n",
    "    auto x_accessor = flat_x.accessor<float, 1>();\n",
    "\n",
    "    auto y = at::empty(x.sizes(), x.options());\n",
    "    auto y_ptr = y.data_ptr<float>();\n",
    "    \n",
    "    auto n = y.numel();\n",
    "    for (int64_t i = 0; i < n; i++) {\n",
    "        *(y_ptr + i) = x_accessor[i] + 1;\n",
    "    }\n",
    "    return y;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print_as_cpp(shift_impl_v1_src)\n",
    "shift_impl_v1 = load_extension(\"shift_impl_v1\", shift_impl_v1_src, \"shift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
