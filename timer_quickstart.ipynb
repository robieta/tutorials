{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before we begin, we need to install torch if it isnâ€™t already available.\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "`conda install pytorch -c pytorch`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;or\n",
    " \n",
    "`pip install torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc\n",
    "\n",
    "We'll start by defining several helper functions which we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import textwrap\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# We want to show certain threading effects, but 1 vs. several dozen\n",
    "# is often too stark a contrast.\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "\n",
    "def print_as_cpp(source: str):\n",
    "    display(Markdown(f\"```c++\\n{source}\\n```\"))\n",
    "\n",
    "\n",
    "def load_extension(name: str, code: str, fn_name: str):\n",
    "   \"\"\"Compile our implementation into an inline module.\n",
    "\n",
    "   Normally we would modify ATen instead, however this allows us\n",
    "   to show an example without having to build PyTorch from source.\n",
    "   \"\"\"\n",
    "   from torch.utils.cpp_extension import load_inline\n",
    "   return load_inline(\n",
    "      name,\n",
    "      code,\n",
    "      extra_cflags=[\"-O2\", \"-g\"],\n",
    "      functions=[fn_name])\n",
    "\n",
    "def module_to_setup_str(m):\n",
    "   \"\"\"Handle importing `m` during Timer setup.\n",
    "\n",
    "   This step is only necessary because we are using custom extensions for\n",
    "   demonstration, rather than modifying and rebuilding PyTorch core.\n",
    "   \"\"\"\n",
    "   module_dir, module_name = os.path.split(m.__file__)\n",
    "   return textwrap.dedent(f\"\"\"\n",
    "      import sys\n",
    "      if not {repr(module_dir)} in sys.path:\n",
    "         sys.path.append({repr(module_dir)})\n",
    "      import {module_name[:-3]} as my_module\n",
    "      \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: a specialized implementation of `x + 1`\n",
    "\n",
    "In this tutorial, we are going to define the `shift` function, and show how to take a systematic approach towards optimizing it. For simplicity, we will only consider float Tensors on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c++\n",
       "\n",
       "// First attempt at a specialized implementation of `x + 1`\n",
       "at::Tensor shift(const at::Tensor & x) {\n",
       "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
       "\n",
       "    auto y = x.clone(at::MemoryFormat::Contiguous);\n",
       "    auto y_ptr = y.data_ptr<float>();\n",
       "    auto n = y.numel();\n",
       "    for (int i = 0; i < n; i++) {\n",
       "        *(y_ptr + i) += 1;\n",
       "    }\n",
       "    return y;\n",
       "}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift_impl_v0_src = \"\"\"\n",
    "// First attempt at a specialized implementation of `x + 1`\n",
    "at::Tensor shift(const at::Tensor & x) {\n",
    "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
    "\n",
    "    auto y = x.clone(at::MemoryFormat::Contiguous);\n",
    "    auto y_ptr = y.data_ptr<float>();\n",
    "    auto n = y.numel();\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        *(y_ptr + i) += 1;\n",
    "    }\n",
    "    return y;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print_as_cpp(shift_impl_v0_src)\n",
    "shift_impl_v0 = load_extension(\"shift_impl_v0\", shift_impl_v0_src, \"shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive benchmark: timeit.Timer\n",
    "\n",
    "### Note: this is just here as a placeholder to help me organize my thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native\n",
      "        n = 1     n = 1024    n = 16384\n",
      "---------------------------------------\n",
      "       7.7 us       8.9 us      14.7 us\n",
      "       8.0 us       9.1 us      14.6 us\n",
      "       7.9 us       8.6 us      14.4 us\n",
      "       8.0 us       8.6 us      14.8 us\n",
      "       8.0 us       9.2 us      14.4 us\n",
      "\n",
      "\n",
      "C++ Extension\n",
      "        n = 1     n = 1024    n = 16384\n",
      "---------------------------------------\n",
      "       4.1 us       5.0 us      21.2 us\n",
      "       6.5 us       7.8 us      22.0 us\n",
      "       4.3 us       5.2 us      17.8 us\n",
      "       4.0 us       5.5 us      17.4 us\n",
      "       4.2 us       5.0 us      17.4 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "repeats = 5\n",
    "sizes = (1, 1024, 16384)\n",
    "\n",
    "\n",
    "def measure_native(n):\n",
    "    num_runs, total_time = timeit.Timer(\n",
    "        \"x + 1\", \n",
    "        setup=f\"import torch;x = torch.ones(({n},))\",\n",
    "    ).autorange()\n",
    "    return total_time / num_runs\n",
    "\n",
    "\n",
    "def measure_cpp(n):\n",
    "    num_runs, total_time = timeit.Timer(\n",
    "        \"shift(x)\", \n",
    "        setup=f\"import torch;x = torch.ones(({n},))\",\n",
    "        globals={\"shift\": shift_impl_v0.shift},\n",
    "    ).autorange()\n",
    "    return total_time / num_runs\n",
    "\n",
    "\n",
    "for title, measure_fn in ((\"Native\", measure_native), (\"\\n\\nC++ Extension\", measure_cpp)):\n",
    "    print(f\"{title}\\n\" + \"\".join([f\"n = {i}\".rjust(13) for i in sizes]) + \"\\n\" + \"-\" * 13 * len(sizes))\n",
    "    for _ in range(repeats):\n",
    "        result_line = \"\"\n",
    "        for n in sizes:\n",
    "            result_line += f\"{measure_fn(n) * 1e6:10.1f} us\"\n",
    "        print(result_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime aware: torch.utils.benchmark.Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fb68cee64e0>\n",
      "x + 1\n",
      "  9.27 us\n",
      "  1 measurement, 100 runs , 1 thread \n",
      "\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fb68d100c50>\n",
      "x + 1\n",
      "  8.59 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "timer = Timer(\n",
    "    stmt=\"x + 1\",\n",
    "    setup=\"x = torch.ones((1,))\",\n",
    ")\n",
    "\n",
    "# The torch utils Timer returns a Measurement object, which contains\n",
    "# metadata about the run as well as replicates, if applicable.\n",
    "print(timer.timeit(100), \"\\n\")\n",
    "\n",
    "\n",
    "m = Timer(\n",
    "    stmt=\"x + 1\",\n",
    "    # Like timeit.Timer, initialization can be done using `setup=...` or `globals=...` (or both).\n",
    "    globals={\"x\": torch.ones((1,))},\n",
    "    \n",
    "    # torch.utils.benchmark.Timer takes several additional annotation argument:\n",
    "    #   label, sub_label, description, and env\n",
    "    # These change the __repr__ measurements, and are used when grouping and displaying\n",
    "    # measurements. (Discussed later.)\n",
    "    label=\"Add one\",\n",
    "    sub_label=\"Generic implementation.\",\n",
    ")\n",
    "\n",
    "print(timer.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer.blocked_autorange\n",
    "### A mixture of timeit.Timer.repeat and timeit.Timer.autorange\n",
    "\n",
    "While `timeit.Timer.autorange` takes a single continuous measurement of at least 0.2 seconds, `torch.utils.benchmark.blocked_autorange` takes many measurements whose times total at least 0.2 seconds (which can be changed by the `min_run_time` parameter) subject to the constraint that timing overhead is a small fraction of the overall measurement. This is acomplished by first running with an increasing number of runs per loop until the run time is much larger than measurement overhead (which also serves as a warm up), and then taking measurements until the target time is reached. This has the useful properties that it wastes less data, and allows us to take statistics in order to assess the reliability of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7fb68d116a90>\n",
      "x + 1\n",
      "  Median: 8.17 us\n",
      "  IQR:    0.40 us (7.96 to 8.36)\n",
      "  25 measurements, 1000 runs per measurement, 1 thread \n",
      "\n",
      "Mean:      8.2 us\n",
      "Median:    8.2 us\n",
      "IQR:       0.4 us\n",
      "Times:  [8.042722940444947e-06, 7.6196156442165375e-06, ..., 8.217006921768188e-06, 8.396882563829423e-06]\n"
     ]
    }
   ],
   "source": [
    "m = Timer(\n",
    "    stmt=\"x + 1\",\n",
    "    setup=\"x = torch.ones((1,))\",\n",
    ").blocked_autorange()\n",
    "\n",
    "# Results summarized by __repr__\n",
    "print(m, \"\\n\")\n",
    "\n",
    "# Helper methods for statistics\n",
    "print(f\"Mean:   {m.mean * 1e6:6.1f} us\")\n",
    "print(f\"Median: {m.median * 1e6:6.1f} us\")\n",
    "print(f\"IQR:    {m.iqr * 1e6:6.1f} us\")\n",
    "print(f\"Times:  {str(m.times[:2])[:-1]}, ..., {str(m.times[-2:])[1:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why runtime awareness matters\n",
    "It's very easy to accidentally make an apples-to-oranges comparizon, such as comparing measurements with different numbers of threads, or forgetting to CUDA synchronize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeit.Timer:                      118 us\n",
      "torch Timer:                       379 us\n",
      "torch Timer(num_threads=...):      115 us\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((1024, 1024))\n",
    "\n",
    "num_runs, total_time = timeit.Timer(\"x + 1\", globals={\"x\": x}).autorange()\n",
    "m0 = Timer(\"x + 1\", globals={\"x\": x}).blocked_autorange()\n",
    "m1 = Timer(\"x + 1\", globals={\"x\": x}, num_threads=torch.get_num_threads()).blocked_autorange()\n",
    "\n",
    "print(f\"timeit.Timer:                   {total_time / num_runs * 1e6:6.0f} us\")\n",
    "print(f\"torch Timer:                    {m0.mean * 1e6:6.0f} us\")\n",
    "print(f\"torch Timer(num_threads=...):   {m1.mean * 1e6:6.0f} us\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.utils.benchmark.Compare\n",
    "Easy comparison of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------- Shift operator ------------------------------------]\n",
      "                               |   1   |   16  |  256  |  1024  |  4096  |  16384  |  32768\n",
      "1 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  7.8  |  7.7  |  8.3  |  8.4   |  9.5   |   14.5  |   21.3\n",
      "      Custom C++ operator      |  3.9  |  4.0  |  4.6  |  4.9   |  8.1   |   18.5  |   32.7\n",
      "2 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  7.8  |  7.8  |  8.4  |  8.6   |  9.5   |   17.1  |   22.5\n",
      "4 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  7.4  |  7.7  |  8.3  |  8.5   |  9.6   |   14.5  |   22.4\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Compare\n",
    "\n",
    "results = []\n",
    "for n in [1, 16, 256, 1024, 4096, 16384, 32768]:\n",
    "    for num_threads in [1, 2, 4]:\n",
    "        setup=f\"x = torch.ones(({n},))\"\n",
    "        results.append(Timer(\n",
    "            \"x + 1\",\n",
    "            setup=setup,\n",
    "            num_threads=num_threads,\n",
    "            label=\"Shift operator\",\n",
    "            sub_label=\"Generic implementation.\",\n",
    "            description=str(n),\n",
    "        ).blocked_autorange())\n",
    "\n",
    "\n",
    "    results.append(Timer(\n",
    "        \"my_module.shift(x)\",\n",
    "        setup=(\n",
    "            module_to_setup_str(shift_impl_v0) +\n",
    "            setup\n",
    "        ),\n",
    "        # Custom C++ operator does not support parallelism.\n",
    "        num_threads=1,\n",
    "        label=\"Shift operator\",\n",
    "        sub_label=\"Custom C++ operator\",\n",
    "        description=str(n),\n",
    "    ).blocked_autorange())\n",
    "\n",
    "compare = Compare(results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With extra formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------- Shift operator ------------------------------------]\n",
      "                               |   1   |   16  |  256  |  1024  |  4096  |  16384  |  32768\n",
      "1 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  7.8  |  7.7  |  8.3  |  8.4   |   10   |  \u001b[92m\u001b[1m  14 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  21 \u001b[0m\u001b[0m\n",
      "      Custom C++ operator      |  \u001b[92m\u001b[1m3.9\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4.0\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4.6\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4.9 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 8  \u001b[0m\u001b[0m  |    19   |    33 \n",
      "2 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  \u001b[92m\u001b[1m7.8\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m7.8\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.4\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.6 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 10 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  20 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  23 \u001b[0m\u001b[0m\n",
      "4 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  \u001b[92m\u001b[1m7.4\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m7.7\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.3\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.5 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 10 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  14 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  22 \u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.utils.benchmark.Fuzzer\n",
    "## More diverse inputs\n",
    "\n",
    "We'll take a brief detour and use fuzzed inputs to discuss transpose and contiguous before returning to `shift`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------- 2D transpose ------------------------------]\n",
      "                                     |  x.contiguous()  |  x.t().contiguous()\n",
      "1 threads: ------------------------------------------------------------------\n",
      "      355    x 355  (discontiguous)  |      210000      |          2000      \n",
      "      751    x 1                     |         200      |          2000      \n",
      "      313    x 313                   |         200      |        160000      \n",
      "      45851  x 1                     |         200      |          2000      \n",
      "      146    x 146                   |         200      |         43000      \n",
      "      15854  x 1                     |         200      |          2000      \n",
      "      143    x 143  (discontiguous)  |       38000      |          2000      \n",
      "      2709   x 1                     |         200      |          2000      \n",
      "      312    x 312                   |         200      |        160000      \n",
      "      5674   x 1                     |         200      |          2000      \n",
      "\n",
      "Times are in nanoseconds (ns).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias\n",
    "\n",
    "example_fuzzer = Fuzzer(\n",
    "    parameters = [\n",
    "        FuzzedParameter(\"k0\", minval=1, maxval=1024 ** 2, distribution=\"loguniform\"),\n",
    "        FuzzedParameter(\"k1\", distribution={1: 0.5, ParameterAlias(\"k0\"): 0.5}, strict=True),\n",
    "    ],\n",
    "    tensors = [\n",
    "        FuzzedTensor(\"x\", size=(\"k0\", \"k1\"), min_elements=128, max_elements=128 * 1024, probability_contiguous=0.6)\n",
    "    ],\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "results = []\n",
    "for tensors, tensor_params, params in example_fuzzer.take(10):\n",
    "    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n",
    "    for stmt in (\"x.contiguous()\", \"x.t().contiguous()\"):\n",
    "        timer = Timer(\n",
    "            stmt,\n",
    "            globals=tensors, \n",
    "            label=\"2D transpose\",\n",
    "            description=stmt,\n",
    "            sub_label=sub_label,\n",
    "        )\n",
    "        results.append(timer.blocked_autorange())\n",
    "\n",
    "compare = Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fuzzed benchmarks reveal several noteworthy features:\n",
    "* If a Tensor is already contiguous we do not need to construct a new Tensor and the operation is extremely cheap. O(100 ns)\n",
    "\n",
    "* For N x 1 Tensors transpose requires that we create a new Tensor, but we can reuse the same buffer.\n",
    "\n",
    "* For N x N tensors, either contiguous or transposed contiguous will be expensive depending on the underlying data layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canned fuzzers: back to our x+1 kernel\n",
    "When benchmarking an op, there are a lot of things to consider: Dimensionality, contiguity (both layout and strides from slicing), broacasting, sizes, etc. While it's certainly possible to write your own fuzzer, it's nice if one doesn't have to. To that end, canned fuzzers are provided for unary and binary ops, and more will be added soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------- Shift operator -------------------------------------------------]\n",
      "                               |  [0]   |  [1]  |  [2]  |   [3]   |  [4]  |  [5]  |  [6]   |  [7]   |   [8]   |  [9]\n",
      "1 threads: ---------------------------------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  \u001b[92m\u001b[1m4000\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 15\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m160\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m40000\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m380\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 94\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4500\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m5600\u001b[0m\u001b[0m  |  80000  |  \u001b[92m\u001b[1m240\u001b[0m\u001b[0m\n",
      "      Custom C++ operator      |  7800  |   17  |  \u001b[2m\u001b[91m450\u001b[0m\u001b[0m  |  49000  |  \u001b[2m\u001b[91m900\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m190\u001b[0m\u001b[0m  |  7000  |  7900  |  \u001b[92m\u001b[1m70000\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m570\u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[0] [16, 34, 8324]      \n",
      "[1] [16384]             \n",
      "[2] [157, 2695]          (discontiguous)\n",
      "[3] [256, 2538, 20]     \n",
      "[4] [960851]            \n",
      "[5] [19, 256, 41]       \n",
      "[6] [30, 131072]        \n",
      "[7] [4096, 855]          (discontiguous)\n",
      "[8] [30018, 16, 24]      (discontiguous)\n",
      "[9] [1286, 487]         \n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark.op_fuzzers import unary\n",
    "\n",
    "results, descriptions = [], []\n",
    "for i, (tensors, tensor_params, params) in enumerate(unary.UnaryOpFuzzer(seed=0).take(10)):\n",
    "    x = tensors[\"x\"]\n",
    "    descriptions.append(f\"{str(list(x.shape)):<20}{'' if tensor_params['x']['is_contiguous'] else ' (discontiguous)'}\")\n",
    "    timer = Timer(\n",
    "        \"x + 1\",\n",
    "        globals=tensors,\n",
    "        label=\"Shift operator\",\n",
    "        sub_label=\"Generic implementation.\",\n",
    "        description=f\"[{i}]\",\n",
    "    )\n",
    "    results.append(timer.blocked_autorange())\n",
    "    \n",
    "    timer = Timer(\n",
    "        \"my_module.shift(x)\",\n",
    "        globals=tensors,\n",
    "        setup=module_to_setup_str(shift_impl_v0),\n",
    "        label=\"Shift operator\",\n",
    "        sub_label=\"Custom C++ operator\",\n",
    "        description=f\"[{i}]\",\n",
    "    )\n",
    "    results.append(timer.blocked_autorange())\n",
    "    \n",
    "compare = Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()\n",
    "\n",
    "for i, d in enumerate(descriptions):\n",
    "    print(f\"[{i}] {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: elaborate on why [2] and [7] are slower than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c++\n",
       "\n",
       "#include <ATen/native/TensorIterator.h>\n",
       "#include <ATen/native/cpu/Loops.h>\n",
       "\n",
       "// Second attempt at a specialized implementation of x + 1\n",
       "at::Tensor shift(const at::Tensor& x) {\n",
       "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
       "\n",
       "    auto result = at::empty_like(x);\n",
       "    auto iter = at::TensorIterator::unary_op(result, x);\n",
       "    at::native::cpu_kernel(iter, [](float xi) -> float { return xi + 1; });\n",
       "\n",
       "    return result;\n",
       "}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift_impl_v1_src = \"\"\"\n",
    "#include <ATen/native/TensorIterator.h>\n",
    "#include <ATen/native/cpu/Loops.h>\n",
    "\n",
    "// Second attempt at a specialized implementation of x + 1\n",
    "at::Tensor shift(const at::Tensor& x) {\n",
    "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
    "\n",
    "    auto result = at::empty_like(x);\n",
    "    auto iter = at::TensorIterator::unary_op(result, x);\n",
    "    at::native::cpu_kernel(iter, [](float xi) -> float { return xi + 1; });\n",
    "\n",
    "    return result;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print_as_cpp(shift_impl_v1_src)\n",
    "shift_impl_v1 = load_extension(\"shift_impl_v1\", shift_impl_v1_src, \"shift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------------------------------------- Shift operator -------------------------------------------------]\n",
      "                                |  [0]   |  [1]  |  [2]  |   [3]   |  [4]  |  [5]  |  [6]   |  [7]   |   [8]   |  [9]\n",
      "1 threads: ----------------------------------------------------------------------------------------------------------\n",
      "      Generic implementation.   |  \u001b[34m\u001b[1m4000\u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m 15\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m160\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m40000\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m380\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 94\u001b[0m\u001b[0m  |  4500  |  5600  |  80000  |  \u001b[92m\u001b[1m240\u001b[0m\u001b[0m\n",
      "      Custom C++ operator       |  7800  |   17  |  \u001b[2m\u001b[91m450\u001b[0m\u001b[0m  |  49000  |  \u001b[2m\u001b[91m900\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m190\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m7000\u001b[0m\u001b[0m  |  7900  |  \u001b[92m\u001b[1m70000\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m570\u001b[0m\u001b[0m\n",
      "      Custom C++ operator (v1)  |  \u001b[92m\u001b[1m4200\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 14\u001b[0m\u001b[0m  |  240  |  44000  |  530  |  120  |  \u001b[92m\u001b[1m3300\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m5100\u001b[0m\u001b[0m  |  \u001b[34m\u001b[1m70000\u001b[0m\u001b[0m  |  340\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[0] [16, 34, 8324]      \n",
      "[1] [16384]             \n",
      "[2] [157, 2695]          (discontiguous)\n",
      "[3] [256, 2538, 20]     \n",
      "[4] [960851]            \n",
      "[5] [19, 256, 41]       \n",
      "[6] [30, 131072]        \n",
      "[7] [4096, 855]          (discontiguous)\n",
      "[8] [30018, 16, 24]      (discontiguous)\n",
      "[9] [1286, 487]         \n"
     ]
    }
   ],
   "source": [
    "# Op fuzzers are deterministic.\n",
    "for i, (tensors, tensor_params, params) in enumerate(unary.UnaryOpFuzzer(seed=0).take(10)):\n",
    "    x = tensors[\"x\"]\n",
    "    d = f\"{str(list(x.shape)):<20}{'' if tensor_params['x']['is_contiguous'] else ' (discontiguous)'}\"\n",
    "    assert d == descriptions[i]\n",
    "\n",
    "    timer = Timer(\n",
    "        \"my_module.shift(x)\",\n",
    "        globals=tensors,\n",
    "        setup=module_to_setup_str(shift_impl_v1),\n",
    "        label=\"Shift operator\",\n",
    "        sub_label=\"Custom C++ operator (v1)\",\n",
    "        description=f\"[{i}]\",\n",
    "    )\n",
    "    results.append(timer.blocked_autorange())\n",
    "    \n",
    "compare = Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()\n",
    "\n",
    "for i, d in enumerate(descriptions):\n",
    "    print(f\"[{i}] {d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
