{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Before we begin, we need to install torch if it isnâ€™t already available.\n",
    "https://pytorch.org/get-started/locally/\n",
    "\n",
    "`conda install pytorch -c pytorch`\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;or\n",
    " \n",
    "`pip install torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc\n",
    "\n",
    "We'll start by defining several helper functions which we'll use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "from typing import List, Union\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# We want to show certain threading effects, but 1 vs. several dozen\n",
    "# is often too stark a contrast.\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "\n",
    "def print_as_cpp(source: str):\n",
    "    display(Markdown(f\"```c++\\n{source}\\n```\"))\n",
    "\n",
    "\n",
    "def load_extension(name: str, code: str, fn_name: Union[str, List[str]]):\n",
    "    \"\"\"Compile our implementation into an inline module.\n",
    "\n",
    "    Normally we would modify ATen instead, however this allows us\n",
    "    to show an example without having to build PyTorch from source.\n",
    "    \"\"\"\n",
    "    from torch.utils.cpp_extension import load_inline\n",
    "    return load_inline(\n",
    "       name,\n",
    "       code,\n",
    "       extra_cflags=[\"-O2\", \"-g\"],\n",
    "       functions=[fn_name] if isinstance(fn_name, str) else fn_name)\n",
    "\n",
    "\n",
    "def module_to_setup_str(m):\n",
    "    \"\"\"Handle importing `m` during Timer setup.\n",
    "\n",
    "    This step is only necessary because we are using custom extensions for\n",
    "    demonstration, rather than modifying and rebuilding PyTorch core.\n",
    "    \"\"\"\n",
    "    module_dir, module_name = os.path.split(m.__file__)\n",
    "    module_name = re.sub(r\"\\.so\", \"\", module_name)\n",
    "\n",
    "    return textwrap.dedent(\n",
    "       f\"\"\"\n",
    "       import sys\n",
    "       if not {repr(module_dir)} in sys.path:\n",
    "          sys.path.append({repr(module_dir)})\n",
    "       import {module_name} as my_module\n",
    "       \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case study: a specialized implementation of `x + 1`\n",
    "\n",
    "In this tutorial, we are going to define the `shift` function, and show how to take a systematic approach towards optimizing it. For simplicity, we will only consider float Tensors on CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c++\n",
       "\n",
       "// First attempt at a specialized implementation of `x + 1`\n",
       "at::Tensor shift(const at::Tensor & x) {\n",
       "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
       "\n",
       "    auto y = x.clone(at::MemoryFormat::Contiguous);\n",
       "    auto y_ptr = x.data_ptr<float>();\n",
       "    auto n = y.numel();\n",
       "    for (int i = 0; i < n; i++) {\n",
       "        *(y_ptr + i) += 1;\n",
       "    }\n",
       "    return y;\n",
       "}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift_impl_v0_src = \"\"\"\n",
    "// First attempt at a specialized implementation of `x + 1`\n",
    "at::Tensor shift(const at::Tensor & x) {\n",
    "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
    "\n",
    "    auto y = x.clone(at::MemoryFormat::Contiguous);\n",
    "    auto y_ptr = x.data_ptr<float>();\n",
    "    auto n = y.numel();\n",
    "    for (int i = 0; i < n; i++) {\n",
    "        *(y_ptr + i) += 1;\n",
    "    }\n",
    "    return y;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print_as_cpp(shift_impl_v0_src)\n",
    "shift_impl_v0 = load_extension(\"shift_impl_v0\", shift_impl_v0_src, \"shift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive benchmark: timeit.Timer\n",
    "\n",
    "### Note: this is just here as a placeholder to help me organize my thoughts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Native\n",
      "        n = 1     n = 1024    n = 16384\n",
      "---------------------------------------\n",
      "       8.3 us       9.0 us      14.2 us\n",
      "       8.3 us       9.0 us      14.3 us\n",
      "       8.5 us       9.2 us      14.5 us\n",
      "       8.2 us       9.1 us      14.4 us\n",
      "       8.4 us       8.9 us      14.4 us\n",
      "\n",
      "\n",
      "C++ Extension\n",
      "        n = 1     n = 1024    n = 16384\n",
      "---------------------------------------\n",
      "       4.3 us       5.4 us      18.2 us\n",
      "       4.2 us       5.2 us      18.0 us\n",
      "       4.3 us       5.5 us      18.4 us\n",
      "       4.4 us       5.6 us      19.1 us\n",
      "       4.7 us       5.6 us      17.2 us\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "repeats = 5\n",
    "sizes = (1, 1024, 16384)\n",
    "\n",
    "\n",
    "def measure_native(n):\n",
    "    num_runs, total_time = timeit.Timer(\n",
    "        \"x + 1\", \n",
    "        setup=f\"import torch;x = torch.ones(({n},))\",\n",
    "    ).autorange()\n",
    "    return total_time / num_runs\n",
    "\n",
    "\n",
    "def measure_cpp(n):\n",
    "    num_runs, total_time = timeit.Timer(\n",
    "        \"shift(x)\", \n",
    "        setup=f\"import torch;x = torch.ones(({n},))\",\n",
    "        globals={\"shift\": shift_impl_v0.shift},\n",
    "    ).autorange()\n",
    "    return total_time / num_runs\n",
    "\n",
    "\n",
    "for title, measure_fn in ((\"Native\", measure_native), (\"\\n\\nC++ Extension\", measure_cpp)):\n",
    "    print(f\"{title}\\n\" + \"\".join([f\"n = {i}\".rjust(13) for i in sizes]) + \"\\n\" + \"-\" * 13 * len(sizes))\n",
    "    for _ in range(repeats):\n",
    "        result_line = \"\"\n",
    "        for n in sizes:\n",
    "            result_line += f\"{measure_fn(n) * 1e6:10.1f} us\"\n",
    "        print(result_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Runtime aware: torch.utils.benchmark.Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f64aa863940>\n",
      "x + 1\n",
      "  9.41 us\n",
      "  1 measurement, 100 runs , 1 thread \n",
      "\n",
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f64aaa78c18>\n",
      "x + 1\n",
      "  7.85 us\n",
      "  1 measurement, 100 runs , 1 thread\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Timer\n",
    "\n",
    "timer = Timer(\n",
    "    stmt=\"x + 1\",\n",
    "    setup=\"x = torch.ones((1,))\",\n",
    ")\n",
    "\n",
    "# The torch utils Timer returns a Measurement object, which contains\n",
    "# metadata about the run as well as replicates, if applicable.\n",
    "print(timer.timeit(100), \"\\n\")\n",
    "\n",
    "\n",
    "m = Timer(\n",
    "    stmt=\"x + 1\",\n",
    "    # Like timeit.Timer, initialization can be done using `setup=...` or `globals=...` (or both).\n",
    "    globals={\"x\": torch.ones((1,))},\n",
    "    \n",
    "    # torch.utils.benchmark.Timer takes several additional annotation argument:\n",
    "    #   label, sub_label, description, and env\n",
    "    # These change the __repr__ measurements, and are used when grouping and displaying\n",
    "    # measurements. (Discussed later.)\n",
    "    label=\"Add one\",\n",
    "    sub_label=\"Generic implementation.\",\n",
    ")\n",
    "\n",
    "print(timer.timeit(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timer.blocked_autorange\n",
    "### A mixture of timeit.Timer.repeat and timeit.Timer.autorange\n",
    "\n",
    "While `timeit.Timer.autorange` takes a single continuous measurement of at least 0.2 seconds, `torch.utils.benchmark.blocked_autorange` takes many measurements whose times total at least 0.2 seconds (which can be changed by the `min_run_time` parameter) subject to the constraint that timing overhead is a small fraction of the overall measurement. This is acomplished by first running with an increasing number of runs per loop until the run time is much larger than measurement overhead (which also serves as a warm up), and then taking measurements until the target time is reached. This has the useful properties that it wastes less data, and allows us to take statistics in order to assess the reliability of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.benchmark.utils.common.Measurement object at 0x7f64aaa44a90>\n",
      "x + 1\n",
      "  Median: 8.11 us\n",
      "  IQR:    0.28 us (7.99 to 8.27)\n",
      "  25 measurements, 1000 runs per measurement, 1 thread \n",
      "\n",
      "Mean:      8.2 us\n",
      "Median:    8.1 us\n",
      "IQR:       0.3 us\n",
      "Times:  [8.668921887874603e-06, 8.090808987617493e-06, ..., 8.229289203882217e-06, 8.40018317103386e-06]\n"
     ]
    }
   ],
   "source": [
    "m = Timer(\n",
    "    stmt=\"x + 1\",\n",
    "    setup=\"x = torch.ones((1,))\",\n",
    ").blocked_autorange()\n",
    "\n",
    "# Results summarized by __repr__\n",
    "print(m, \"\\n\")\n",
    "\n",
    "# Helper methods for statistics\n",
    "print(f\"Mean:   {m.mean * 1e6:6.1f} us\")\n",
    "print(f\"Median: {m.median * 1e6:6.1f} us\")\n",
    "print(f\"IQR:    {m.iqr * 1e6:6.1f} us\")\n",
    "print(f\"Times:  {str(m.times[:2])[:-1]}, ..., {str(m.times[-2:])[1:]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why runtime awareness matters\n",
    "It's very easy to accidentally make an apples-to-oranges comparizon, such as comparing measurements with different numbers of threads, or forgetting to CUDA synchronize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeit.Timer:                      113 us\n",
      "torch Timer:                       372 us\n",
      "torch Timer(num_threads=...):      114 us\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones((1024, 1024))\n",
    "\n",
    "num_runs, total_time = timeit.Timer(\"x + 1\", globals={\"x\": x}).autorange()\n",
    "m0 = Timer(\"x + 1\", globals={\"x\": x}).blocked_autorange()\n",
    "m1 = Timer(\"x + 1\", globals={\"x\": x}, num_threads=torch.get_num_threads()).blocked_autorange()\n",
    "\n",
    "print(f\"timeit.Timer:                   {total_time / num_runs * 1e6:6.0f} us\")\n",
    "print(f\"torch Timer:                    {m0.mean * 1e6:6.0f} us\")\n",
    "print(f\"torch Timer(num_threads=...):   {m1.mean * 1e6:6.0f} us\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.utils.benchmark.Compare\n",
    "Easy comparison of measurements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------- Shift operator ------------------------------------]\n",
      "                               |   1   |   16  |  256  |  1024  |  4096  |  16384  |  32768\n",
      "1 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  7.8  |  8.2  |  8.6  |  8.7   |  10.0  |   13.8  |   21.8\n",
      "      Custom C++ operator      |  4.4  |  4.4  |  4.8  |  5.4   |   8.4  |   17.7  |   33.7\n",
      "2 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  8.1  |  8.0  |  8.7  |  8.9   |  10.1  |   13.9  |   22.9\n",
      "4 threads: --------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  8.1  |  8.1  |  8.6  |  8.6   |   9.9  |   14.1  |   22.9\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Compare\n",
    "\n",
    "results = []\n",
    "for n in [1, 16, 256, 1024, 4096, 16384, 32768]:\n",
    "    for num_threads in [1, 2, 4]:\n",
    "        setup=f\"x = torch.ones(({n},))\"\n",
    "        results.append(Timer(\n",
    "            \"x + 1\",\n",
    "            setup=setup,\n",
    "            num_threads=num_threads,\n",
    "            label=\"Shift operator\",\n",
    "            sub_label=\"Generic implementation.\",\n",
    "            description=str(n),\n",
    "        ).blocked_autorange())\n",
    "\n",
    "\n",
    "    results.append(Timer(\n",
    "        \"my_module.shift(x)\",\n",
    "        setup=(\n",
    "            module_to_setup_str(shift_impl_v0) +\n",
    "            setup\n",
    "        ),\n",
    "        # Custom C++ operator does not support parallelism.\n",
    "        num_threads=1,\n",
    "        label=\"Shift operator\",\n",
    "        sub_label=\"Custom C++ operator\",\n",
    "        description=str(n),\n",
    "    ).blocked_autorange())\n",
    "\n",
    "compare = Compare(results)\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With extra formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------ Shift operator -----------------------------------]\n",
      "                               |  1  |   16  |  256  |  1024  |  4096  |  16384  |  32768\n",
      "1 threads: ------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  8  |  8.2  |  8.6  |   9    |   10   |  \u001b[92m\u001b[1m  14 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  22 \u001b[0m\u001b[0m\n",
      "      Custom C++ operator      |  \u001b[92m\u001b[1m4\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4.4\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4.8\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 5  \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  8 \u001b[0m\u001b[0m  |    18   |    34 \n",
      "2 threads: ------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  \u001b[92m\u001b[1m8\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.0\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.7\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 9  \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 10 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  14 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  23 \u001b[0m\u001b[0m\n",
      "4 threads: ------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  \u001b[92m\u001b[1m8\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.1\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m8.6\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 9  \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 10 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  14 \u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m  23 \u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch.utils.benchmark.Fuzzer\n",
    "## More diverse inputs\n",
    "\n",
    "We'll take a brief detour and use fuzzed inputs to discuss transpose and contiguous before returning to `shift`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------- 2D transpose ------------------------------]\n",
      "                                     |  x.contiguous()  |  x.t().contiguous()\n",
      "1 threads: ------------------------------------------------------------------\n",
      "      355    x 355  (discontiguous)  |      200000      |          2000      \n",
      "      751    x 1                     |         200      |          2000      \n",
      "      313    x 313                   |         200      |        170000      \n",
      "      45851  x 1                     |         200      |          2000      \n",
      "      146    x 146                   |         200      |         43000      \n",
      "      15854  x 1                     |         200      |          2000      \n",
      "      143    x 143  (discontiguous)  |       40000      |          2000      \n",
      "      2709   x 1                     |         200      |          2000      \n",
      "      312    x 312                   |         200      |        170000      \n",
      "      5674   x 1                     |         200      |          2000      \n",
      "\n",
      "Times are in nanoseconds (ns).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Fuzzer, FuzzedParameter, FuzzedTensor, ParameterAlias\n",
    "\n",
    "example_fuzzer = Fuzzer(\n",
    "    parameters = [\n",
    "        FuzzedParameter(\"k0\", minval=1, maxval=1024 ** 2, distribution=\"loguniform\"),\n",
    "        FuzzedParameter(\"k1\", distribution={1: 0.5, ParameterAlias(\"k0\"): 0.5}, strict=True),\n",
    "    ],\n",
    "    tensors = [\n",
    "        FuzzedTensor(\"x\", size=(\"k0\", \"k1\"), min_elements=128, max_elements=128 * 1024, probability_contiguous=0.6)\n",
    "    ],\n",
    "    seed=0,\n",
    ")\n",
    "\n",
    "results = []\n",
    "for tensors, tensor_params, params in example_fuzzer.take(10):\n",
    "    sub_label=f\"{params['k0']:<6} x {params['k1']:<4} {'' if tensor_params['x']['is_contiguous'] else '(discontiguous)'}\"\n",
    "    for stmt in (\"x.contiguous()\", \"x.t().contiguous()\"):\n",
    "        timer = Timer(\n",
    "            stmt,\n",
    "            globals=tensors, \n",
    "            label=\"2D transpose\",\n",
    "            description=stmt,\n",
    "            sub_label=sub_label,\n",
    "        )\n",
    "        results.append(timer.blocked_autorange())\n",
    "\n",
    "compare = Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fuzzed benchmarks reveal several noteworthy features:\n",
    "* If a Tensor is already contiguous we do not need to construct a new Tensor and the operation is extremely cheap. O(100 ns)\n",
    "\n",
    "* For N x 1 Tensors transpose requires that we create a new Tensor, but we can reuse the same buffer.\n",
    "\n",
    "* For N x N tensors, either contiguous or transposed contiguous will be expensive depending on the underlying data layout."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Canned fuzzers: back to our x+1 kernel\n",
    "When benchmarking an op, there are a lot of things to consider: Dimensionality, contiguity (both layout and strides from slicing), broacasting, sizes, etc. While it's certainly possible to write your own fuzzer, it's nice if one doesn't have to. To that end, canned fuzzers are provided for unary and binary ops, and more will be added soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------------- Shift operator -------------------------------------------------]\n",
      "                               |  [0]   |  [1]  |  [2]  |   [3]   |  [4]  |  [5]  |  [6]   |  [7]   |   [8]   |  [9]\n",
      "1 threads: ---------------------------------------------------------------------------------------------------------\n",
      "      Generic implementation.  |  \u001b[92m\u001b[1m3300\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 14\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m160\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m34000\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m340\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 78\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m3000\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4900\u001b[0m\u001b[0m  |  90000  |  \u001b[92m\u001b[1m240\u001b[0m\u001b[0m\n",
      "      Custom C++ operator      |  6000  |   18  |  \u001b[2m\u001b[91m470\u001b[0m\u001b[0m  |  50000  |  \u001b[2m\u001b[91m830\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m180\u001b[0m\u001b[0m  |  4700  |  6700  |  \u001b[92m\u001b[1m70000\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m580\u001b[0m\u001b[0m\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[0] [16, 34, 8324]      \n",
      "[1] [16384]             \n",
      "[2] [157, 2695]          (discontiguous)\n",
      "[3] [256, 2538, 20]     \n",
      "[4] [960851]            \n",
      "[5] [19, 256, 41]       \n",
      "[6] [30, 131072]        \n",
      "[7] [4096, 855]          (discontiguous)\n",
      "[8] [30018, 16, 24]      (discontiguous)\n",
      "[9] [1286, 487]         \n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark.op_fuzzers import unary\n",
    "\n",
    "results, descriptions = [], []\n",
    "for i, (tensors, tensor_params, params) in enumerate(unary.UnaryOpFuzzer(seed=0).take(10)):\n",
    "    x = tensors[\"x\"]\n",
    "    descriptions.append(f\"{str(list(x.shape)):<20}{'' if tensor_params['x']['is_contiguous'] else ' (discontiguous)'}\")\n",
    "    timer = Timer(\n",
    "        \"x + 1\",\n",
    "        globals=tensors,\n",
    "        label=\"Shift operator\",\n",
    "        sub_label=\"Generic implementation.\",\n",
    "        description=f\"[{i}]\",\n",
    "    )\n",
    "    results.append(timer.blocked_autorange())\n",
    "    \n",
    "    timer = Timer(\n",
    "        \"my_module.shift(x)\",\n",
    "        globals=tensors,\n",
    "        setup=module_to_setup_str(shift_impl_v0),\n",
    "        label=\"Shift operator\",\n",
    "        sub_label=\"Custom C++ operator\",\n",
    "        description=f\"[{i}]\",\n",
    "    )\n",
    "    results.append(timer.blocked_autorange())\n",
    "    \n",
    "compare = Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()\n",
    "\n",
    "for i, d in enumerate(descriptions):\n",
    "    print(f\"[{i}] {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: elaborate on why [2] and [7] are slower than expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c++\n",
       "\n",
       "#include <ATen/native/TensorIterator.h>\n",
       "#include <ATen/native/cpu/Loops.h>\n",
       "\n",
       "// Second attempt at a specialized implementation of x + 1\n",
       "at::Tensor shift(const at::Tensor& x) {\n",
       "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
       "\n",
       "    auto result = at::empty_like(x);\n",
       "    if (x.is_contiguous()) {\n",
       "        // Fast path for contiguous inputs.\n",
       "        auto x_ptr = x.data_ptr<float>();\n",
       "        auto result_ptr = result.data_ptr<float>();\n",
       "        auto n = x.numel();\n",
       "\n",
       "        for (int64_t i = 0; i < n; ++i, ++x_ptr, ++result_ptr) {\n",
       "           *result_ptr = *x_ptr + 1;\n",
       "        }\n",
       "    } else {\n",
       "        // Fall back to more general machinery if x is discontiguous.\n",
       "        auto iter = at::TensorIterator::unary_op(result, x);\n",
       "        at::native::cpu_kernel(iter, [](float xi) -> float { return xi + 1; });\n",
       "    }\n",
       "    \n",
       "    \n",
       "    return result;\n",
       "}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift_impl_v1_src = \"\"\"\n",
    "#include <ATen/native/TensorIterator.h>\n",
    "#include <ATen/native/cpu/Loops.h>\n",
    "\n",
    "// Second attempt at a specialized implementation of x + 1\n",
    "at::Tensor shift(const at::Tensor& x) {\n",
    "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
    "\n",
    "    auto result = at::empty_like(x);\n",
    "    if (x.is_contiguous()) {\n",
    "        // Fast path for contiguous inputs.\n",
    "        auto x_ptr = x.data_ptr<float>();\n",
    "        auto result_ptr = result.data_ptr<float>();\n",
    "        auto n = x.numel();\n",
    "\n",
    "        for (int64_t i = 0; i < n; ++i, ++x_ptr, ++result_ptr) {\n",
    "           *result_ptr = *x_ptr + 1;\n",
    "        }\n",
    "    } else {\n",
    "        // Fall back to more general machinery if x is discontiguous.\n",
    "        auto iter = at::TensorIterator::unary_op(result, x);\n",
    "        at::native::cpu_kernel(iter, [](float xi) -> float { return xi + 1; });\n",
    "    }\n",
    "    \n",
    "    \n",
    "    return result;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "print_as_cpp(shift_impl_v1_src)\n",
    "shift_impl_v1 = load_extension(\"shift_impl_v1\", shift_impl_v1_src, \"shift\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------------------------------------- Shift operator -------------------------------------------------]\n",
      "                                |  [0]   |  [1]  |  [2]  |   [3]   |  [4]  |  [5]  |  [6]   |  [7]   |   [8]   |  [9]\n",
      "1 threads: ----------------------------------------------------------------------------------------------------------\n",
      "      Generic implementation.   |  \u001b[92m\u001b[1m3300\u001b[0m\u001b[0m  |   14  |  \u001b[92m\u001b[1m160\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m34000\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m340\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m 78\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m3000\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m4900\u001b[0m\u001b[0m  |  90000  |  \u001b[92m\u001b[1m240\u001b[0m\u001b[0m\n",
      "      Custom C++ operator       |  6000  |   18  |  \u001b[2m\u001b[91m470\u001b[0m\u001b[0m  |  50000  |  \u001b[2m\u001b[91m830\u001b[0m\u001b[0m  |  \u001b[2m\u001b[91m180\u001b[0m\u001b[0m  |  4700  |  6700  |  70000  |  \u001b[2m\u001b[91m580\u001b[0m\u001b[0m\n",
      "      Custom C++ operator (v1)  |  4200  |  \u001b[92m\u001b[1m 11\u001b[0m\u001b[0m  |  240  |  39000  |  490  |  103  |  3600  |  \u001b[34m\u001b[1m5000\u001b[0m\u001b[0m  |  \u001b[92m\u001b[1m62000\u001b[0m\u001b[0m  |  300\n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[0] [16, 34, 8324]      \n",
      "[1] [16384]             \n",
      "[2] [157, 2695]          (discontiguous)\n",
      "[3] [256, 2538, 20]     \n",
      "[4] [960851]            \n",
      "[5] [19, 256, 41]       \n",
      "[6] [30, 131072]        \n",
      "[7] [4096, 855]          (discontiguous)\n",
      "[8] [30018, 16, 24]      (discontiguous)\n",
      "[9] [1286, 487]         \n"
     ]
    }
   ],
   "source": [
    "# Op fuzzers are deterministic.\n",
    "for i, (tensors, tensor_params, params) in enumerate(unary.UnaryOpFuzzer(seed=0).take(10)):\n",
    "    x = tensors[\"x\"]\n",
    "    d = f\"{str(list(x.shape)):<20}{'' if tensor_params['x']['is_contiguous'] else ' (discontiguous)'}\"\n",
    "    assert d == descriptions[i]\n",
    "\n",
    "    timer = Timer(\n",
    "        \"my_module.shift(x)\",\n",
    "        globals=tensors,\n",
    "        setup=module_to_setup_str(shift_impl_v1),\n",
    "        label=\"Shift operator\",\n",
    "        sub_label=\"Custom C++ operator (v1)\",\n",
    "        description=f\"[{i}]\",\n",
    "    )\n",
    "    results.append(timer.blocked_autorange())\n",
    "    \n",
    "compare = Compare(results)\n",
    "compare.trim_significant_figures()\n",
    "compare.colorize()\n",
    "compare.print()\n",
    "\n",
    "for i, d in enumerate(descriptions):\n",
    "    print(f\"[{i}] {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecting small changes with Callgrind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c++\n",
       "#include <ATen/native/TensorIterator.h>\n",
       "#include <ATen/native/cpu/Loops.h>\n",
       "\n",
       "// Second attempt at a specialized implementation of x + 1\n",
       "at::Tensor shift(const at::Tensor& x) {\n",
       "    TORCH_CHECK(x.scalar_type() == at::kFloat, \"shift requires a float input\");\n",
       "\n",
       "    auto result = at::empty_like(x);\n",
       "    if (x.is_contiguous()) {\n",
       "        // Fast path for contiguous inputs.\n",
       "        auto x_ptr = x.data_ptr<float>();\n",
       "        auto result_ptr = result.data_ptr<float>();\n",
       "        auto n = x.numel();\n",
       "\n",
       "        for (int64_t i = 0; i < n; ++i, ++x_ptr, ++result_ptr) {\n",
       "           *result_ptr = *x_ptr + 1;\n",
       "        }\n",
       "    } else {\n",
       "        // Fall back to more general machinery if x is discontiguous.\n",
       "        auto iter = at::TensorIterator::unary_op(result, x);\n",
       "        at::native::cpu_kernel(iter, [](float xi) -> float { return xi + 1; });\n",
       "    }\n",
       "    \n",
       "    \n",
       "    return result;\n",
       "}\n",
       "at::Tensor shift_with_bailout(const at::Tensor& x) {\n",
       "    return x.numel() ? shift(x) : at::empty(0, x.options());\n",
       "}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "shift_impl_v2_src = f\"\"\"{shift_impl_v1_src.strip()}\n",
    "at::Tensor shift_with_bailout(const at::Tensor& x) {{\n",
    "    return x.numel() ? shift(x) : at::empty({0}, x.options());\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "print_as_cpp(shift_impl_v2_src)\n",
    "shift_impl_v2 = load_extension(\"shift_impl_v2\", shift_impl_v2_src, [\"shift\", \"shift_with_bailout\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_counts_and_times(fn: str, n: int):\n",
    "    timer = Timer(\n",
    "        f\"my_module.{fn}(x)\",\n",
    "        setup=(\n",
    "            module_to_setup_str(shift_impl_v2) +\n",
    "            f\"x = torch.ones(({n},))\"\n",
    "        ))\n",
    "    return (\n",
    "        # Run for a long time to get robust statistics.\n",
    "        timer.blocked_autorange(min_run_time=20),\n",
    "        timer.collect_callgrind()\n",
    "    )\n",
    "\n",
    "\n",
    "def trim_count_repr(c):\n",
    "    setup_str = textwrap.indent(module_to_setup_str(shift_impl_v2), \" \")\n",
    "    abridged_str = \"\\n import ... as my_module\\n\"\n",
    "    return repr(c).replace(setup_str, abridged_str)\n",
    "\n",
    "\n",
    "def delta(counts_0, counts_1):\n",
    "    for c, fn in counts_1.as_standardized().delta(counts_0.as_standardized()):\n",
    "        if \"lookdict_unicode_nodummy\" in fn:\n",
    "            continue  # This is a noisy method in the Python interpreter.\n",
    "            \n",
    "        # Trim down some task specific prefixes to make them easier to read\n",
    "        fn = re.sub(r\"^.+torch_extensions/\", \"\", fn)\n",
    "        fn = re.sub(f\"^.+site-packages/\", \"\", fn)\n",
    "        yield c, fn\n",
    "        \n",
    "\n",
    "def render_fast_path_effect(n: int):\n",
    "    times, counts = collect_counts_and_times(\"shift\", n)\n",
    "    times_with_bailout, counts_with_bailout = collect_counts_and_times(\"shift_with_bailout\", n)\n",
    "    \n",
    "    print(f\"{'-' * 30}\\n-- x = torch.ones(({n},)) ------\\n{'-' * 30}\\n\")\n",
    "    print(\"shift(x)\")\n",
    "    print(\"\\n\".join(repr(times).splitlines()[2:]), \"\\n\")\n",
    "    print(\"\\n\".join(trim_count_repr(counts).splitlines()[6:]))\n",
    "    print(f\"\\n{'-' * 80}\\n\")\n",
    "    print(\"shift_with_bailout(x)\")\n",
    "    print(\"\\n\".join(repr(times_with_bailout).splitlines()[2:]), \"\\n\")\n",
    "    print(\"\\n\".join(trim_count_repr(counts_with_bailout).splitlines()[6:]), \"\\n\" * 3)\n",
    "\n",
    "    lines = []\n",
    "    for c, fn in delta(counts, counts_with_bailout):\n",
    "        line = f\"{c:>8} {fn}\"\n",
    "        lines.append(line[:110] + \"...\" if len(line) > 110 else line)\n",
    "\n",
    "    print(\"Instruction deltas:\")\n",
    "    lines = lines if len(lines) < 16 else lines[:8] + [\"...\"] + lines[-8:]\n",
    "    for l in lines:\n",
    "        print(l)\n",
    "\n",
    "    difference = (\n",
    "        counts_with_bailout.counts(include_lookdict_unicode=False) / \n",
    "        counts.counts(include_lookdict_unicode=False))\n",
    "    print(f\"\\n\\n{(difference - 1) * 100:5.1f}% difference in instruction count.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When the n=0 fast path is triggered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "-- x = torch.ones((0,)) ------\n",
      "------------------------------\n",
      "\n",
      "shift(x)\n",
      "  Median: 2.29 us\n",
      "  IQR:    0.10 us (2.24 to 2.33)\n",
      "  870 measurements, 10000 runs per measurement, 1 thread \n",
      "\n",
      "                         All          Noisy symbols removed\n",
      "  Instructions:       977221                     963223\n",
      "  Baseline:             4610                       4180\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "shift_with_bailout(x)\n",
      "  Median: 1.48 us\n",
      "  IQR:    0.09 us (1.45 to 1.54)\n",
      "  1259 measurements, 10000 runs per measurement, 1 thread \n",
      "\n",
      "                         All          Noisy symbols removed\n",
      "  Instructions:       672488                     658523\n",
      "  Baseline:             4610                       4180 \n",
      "\n",
      "\n",
      "\n",
      "Instruction deltas:\n",
      "    9600 build/../aten/src/ATen/core/dispatch/Dispatcher.h:at::Tensor c10::Dispatcher::callWithDispatchKey<at:...\n",
      "    3300 build/aten/src/ATen/BackendSelectRegister.cpp:at::(anonymous namespace)::empty_memory_format(c10::Arr...\n",
      "    3300 build/aten/src/ATen/BackendSelectRegister.cpp:at::(anonymous namespace)::empty_memory_format(c10::Arr...\n",
      "    3300 build/../aten/src/ATen/core/boxing/KernelFunction_impl.h:at::Tensor c10::Dispatcher::callWithDispatch...\n",
      "    3200 build/../c10/core/TensorOptions.h:c10::TensorOptions::computeDispatchKey() const\n",
      "    3200 build/../aten/src/ATen/core/dispatch/Dispatcher.h:at::Tensor c10::Dispatcher::callWithDispatchKey<at:...\n",
      "    3100 build/../torch/csrc/autograd/generated/VariableType_4.cpp:torch::autograd::VariableType::(anonymous n...\n",
      "    2700 build/../c10/util/Optional.h:at::Tensor c10::Dispatcher::callWithDispatchKey<at::Tensor, c10::ArrayRe...\n",
      "...\n",
      "   -7200 build/../c10/core/Device.h:c10::Device::validate()\n",
      "   -7200 build/../aten/src/ATen/core/boxing/impl/make_boxed_from_unboxed_functor.h:c10::impl::wrap_kernel_func...\n",
      "   -7500 build/../aten/src/ATen/core/boxing/KernelFunction_impl.h:at::Tensor c10::Dispatcher::callWithDispatch...\n",
      "  -10200 build/../c10/util/Optional.h:c10::impl::wrap_kernel_functor_unboxed_<c10::impl::detail::WrapFunctionI...\n",
      "  -12000 build/../aten/src/ATen/core/dispatch/Dispatcher.h:at::Tensor c10::Dispatcher::callWithDispatchKey<at:...\n",
      "  -12300 build/../c10/util/Optional.h:at::Tensor c10::Dispatcher::callWithDispatchKey<at::Tensor, c10::ArrayRe...\n",
      "  -16000 ???:_dl_update_slotinfo\n",
      "  -30400 build/../c10/core/TensorOptions.h:c10::TensorOptions::merge_in(c10::TensorOptions) const\n",
      "\n",
      "\n",
      "-31.6% difference in instruction count.\n"
     ]
    }
   ],
   "source": [
    "render_fast_path_effect(n=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect on the normal path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "-- x = torch.ones((1,)) ------\n",
      "------------------------------\n",
      "\n",
      "shift(x)\n",
      "  Median: 2.65 us\n",
      "  IQR:    0.09 us (2.61 to 2.70)\n",
      "  739 measurements, 10000 runs per measurement, 1 thread \n",
      "\n",
      "                         All          Noisy symbols removed\n",
      "  Instructions:      1111799                    1097920\n",
      "  Baseline:             4610                       4180\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "shift_with_bailout(x)\n",
      "  Median: 2.71 us\n",
      "  IQR:    0.10 us (2.67 to 2.76)\n",
      "  723 measurements, 10000 runs per measurement, 1 thread \n",
      "\n",
      "                         All          Noisy symbols removed\n",
      "  Instructions:      1118075                    1099974\n",
      "  Baseline:             4610                       4180 \n",
      "\n",
      "\n",
      "\n",
      "Instruction deltas:\n",
      "    1300 shift_impl_v2/main.cpp:shift_with_bailout(at::Tensor const&)\n",
      "     800 torch/include/ATen/core/TensorBody.h:shift_with_bailout(at::Tensor const&)\n",
      "     400 build/../c10/core/TensorImpl.h:c10::TensorImpl::numel() const\n",
      "      51 ???:_int_malloc\n",
      "     -41 ???:_int_memalign\n",
      "     -56 ???:_int_free\n",
      "    -200 build/../c10/core/Allocator.h:c10::TensorImpl::data() const\n",
      "    -200 build/../aten/src/TH/THAllocator.cpp:getTHDefaultAllocator()\n",
      "\n",
      "\n",
      "  0.2% difference in instruction count.\n"
     ]
    }
   ],
   "source": [
    "render_fast_path_effect(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a difference, but with wall clock it's difficult to differentiate minor regressions from noise. Instruction counts, by contrast, are deterministic and highlight not only how much more work is being done but also where it is being done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
